---
title: "Computating summary of BART and random forest"
date: '2023-04-25'
output:
  html_document:
    keep_md: yes
---


#### Load and prepare posterior simulation of BART test data 
Each file represents test data for each participant by leave-one-out CV.   
The files are generated by the script, BART_BatchCode.R
Unit of analysis is 5 second time points 
```{r setup, warning=FALSE, message=FALSE, results='hide'}

rm(list = ls())

library(stringr)
library(tidyverse)
library(parallel)
library(data.table)
library(caret)
library(scales)
library(parallel)
library(htmlTable)
library(kableExtra)
library(mlr3measures)


# Plot format
theme_set(theme_classic())
theme_update(
  text = element_text(size = 10), 
  plot.title = element_text(size = 10), 
  legend.text = element_text(size = 10), 
  legend.title = element_text(size = 10)
)

# Function to process each file
funcLoad <- function(filePath, element){
  cat(c("loading -----", filePath))
  d <- readRDS(filePath)
  return(d[element])
}

# Functions to compile outputs from BART 
source("functions/funcBARTPosterior.R")

# Names of Classes to appear in the results 
className <-  c("Lying", "Sitting", "Walking", "Running_3METs", "Running_5METs", "Running_7METs")

# Function to calculate accuracy and 95%CI per class for all participants - not needed for the updated script during revision 
funcGetPostAccClass <- function(df){
  df <- df %>%  
    group_by(mcmcItr) %>% 
    group_by(obsClassLabel, .add = TRUE) %>% 
    summarise(acc = mean(matchMCMC)) 

  df %>% 
    group_by(obsClassLabel) %>%  
    summarise(Mean = mean(acc),
              CILower = quantile(acc, probs= c(0.025)), 
              CIUpper = quantile(acc, probs= c(0.975))) %>%  
    mutate_if(is.numeric, round, 2) %>% 
    mutate_if(is.numeric, unname) %>% 
    rename(Class = obsClassLabel) %>% 
    return()
}

```



```{r data, results='hide'}
# Name files ---------
# list of filenames generated by BART_run script - there are 37 files, each representing leave-one-out-CV prediction from 37 people.  
listPostFile_parallel <- list.files("data_output/Parallel", pattern="BART_PostSimTestTrain", full.names=TRUE)

# Extract file IDs 
nameIndex<- str_replace(listPostFile_parallel, '.+loocv_(.+)', '\\1') %>% 
  str_replace('(.+).rds', '\\1') 

# is there duplicated person ID?
nameIndex[str_remove(nameIndex, "_ArrayIndex_\\d+") %>%  duplicated ]


# Load data ------------
# load data element 2 (Prediction from test data, first element is prediction from training data) 
bool_loadPostFiles = FALSE # 
if(bool_loadPostFiles){ # If true, load individual files and combine - takes time   
  b <- lapply(listPostFile_parallel, funcLoad, 2) 
  names(b) <- listPostFile_parallel
  saveRDS(b, "data_output/Parallel/b.rds")
}else{# load already combined file 
  b <- readRDS("data_output/Parallel/b.rds") # loading pre-saved file 
}


# Information about participant demographics -- these data are stripped from the public version  
#demogTable <- readRDS("data/demogTable.rds")

# Names ---------------------------------------------
# check files and shorten list name - one person was accidentally duplicated 
bNamesIndex <- names(b) %>% str_replace('.+loocv_(.+)', '\\1') %>% 
  str_replace('(.+).rds', '\\1') 

str_remove(bNamesIndex, "_ArrayIndex_\\d+") %>%  
  duplicated %>%  
  sum

# Are there duplicated person ID (tested twice by accident)
bNamesIndex[
  str_remove(bNamesIndex, "_ArrayIndex_\\d+") %>%  duplicated]
names(b) <- bNamesIndex


# Combine everyone as data.table 
bDt <- bind_rows(b, .id = "personID")
```



#### Participant information (data not available publicly)
Demographic and biological information will be removed in the paper unless aggregated, for anonymity.  
`UnitAnalysis` indicates the number of time points, 5 sec window.
`rowTime` indicates the numner of row time points (30 HZ, not aggregated at 5 second window)
```{r, results='hide'}
data.frame(
  PersonID = as.integer(str_remove(names(b), "_ArrayIndex_\\d+")), 
  UnitAnalysis = lapply(b, function(x) length(unique(x$windowID))) %>% unlist 
) %>% 
  left_join(demogTable, by = c("PersonID"="participant_id")) %>% 
  rename(rowTime = countTime)  %>% 
  select(-gender, -age, -height, -weight)
```







### Prep for data - run confusion Mx codes to generate summary metrics and data for CMX 
```{r}
#Split by MCMC sample and compute CMX in each sample  
bDtSplit <- split(bDt, list(bDt$mcmcItr))
funcCMX <- function(x){return(confusionMatrix(factor(x$maxProbClass), factor(x$obsClass), mode="everything"))}

# Each MCMC has table (confusion matrix)
cmx <- mclapply(bDtSplit, funcCMX)
# Concert CX table into df 
c <- mclapply(cmx, function(x) x$table %>% data.frame) 

# Combine all cMX , each CX data frame has a column of prediciton label, ref label, and pairwise frequency (match)
c <- c %>% reduce(left_join, by = c("Prediction","Reference"))
# columne to n = 6*6 classes, m = 2000 itr + prediction label, reference label
colnames(c) <- c("Prediction","Reference", 1:{ncol(c)-2})
```




### Prep CX     
  Top MX  Posterior mean of agreement for each cross-classification - not different form the standard confusion MX      
  Bottom MX   Posterior 95% Credible interval for agreement 
```{r}
# Create summary stats data frame across all prediction/observation pairs 
# CI is baed on the distribution of matched count 
cSummary <- bind_cols(c[,1:2], Mean = rowMeans(c[, 3:ncol(c)])) %>% 
  bind_cols(
    apply(c[, 3:ncol(c)], 1, quantile, probs=c(0.025, 0.975)) %>%  
    t() %>% 
    data.frame() %>% 
    rename(L = X2.5., U =X97.5.) %>% 
    mutate(Range = U-L, CI = paste(round(L, 2), "-",round(U,2), sep =""))
    )

# Create data frame to label class 
levels(bDt$obsClassLabel) <- className
nameDf <- data.frame(
  classLabel = unique(bDt$obsClassLabel), 
  id = factor(1:6)
  )

# Rename columns for figure label 
cSummary <- cSummary %>% 
  left_join(nameDf, by =c("Prediction"="id")) %>% 
  rename(Predicted = classLabel) %>%  
  left_join(nameDf, by =c("Reference"="id")) %>% 
  rename(Observed = classLabel)
```


### CX, Posterior mean and 95% CI 
```{r}
# Now plot 
# Posterior mean - standard Mx, except cell values are sum of probabilities, not 1/0 - probably two show very similar results 
cSummary %>%
  select(Predicted, Observed, Mean) %>% 
  mutate(Mean = round(Mean, 2)) %>% 
  ggplot2::ggplot(aes(Predicted, Observed)) +
  #geom_tile(aes(fill = Mean), colour = "gray50") +
  #scale_fill_gradient(low = "white", high = muted("gray80")) +
  geom_tile(aes(fill = Mean), colour = "black") +
  scale_fill_gradient(low = "white", high = "grey30", name = "match") +
  geom_text(aes(label = Mean), size=6) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
    text = element_text(size = 20),  
    legend.text = element_text(size = 20), 
    legend.title = element_text(size = 20)) +
  ggtitle("Posterior mean, confusion MX")


# posterior 95% interval for cells 
cSummary %>%
  select(Predicted, Observed , Range, CI) %>% 
  mutate(Range = round(Range, 2)) %>% 
  ggplot2::ggplot(aes(Predicted, Observed)) +
  geom_tile(aes(fill = Range), colour = "gray50") +
  scale_fill_gradient(low = "white", high = muted("gray80")) +
  geom_text(aes(label = CI)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Posteiror 95% CI of agreement")



```   




### F1 score histogram 
```{r}
postF1 <- lapply(cmx, function(x) x$byClass %>%  data.frame() %>% select(F1) %>% t)
postF1 <- do.call(rbind, postF1) 
colnames(postF1) <-  className

postF1T <- postF1 %>% 
  data.frame() %>% 
  tidyr::gather(Activity_type, value)

postF1T <- postF1T %>% 
  dplyr::mutate(Activity_type = factor(Activity_type, levels=className, labels=className)) %>% 
  data.frame()

ggplot(data = postF1T, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ Activity_type, ncol = 1)  + 
  theme_classic() + 
  ggtitle("B") + xlab("Posterior distribution of F1 score")


# Compute the posterior summary - mean and interval
apply(postF1, 2, median) %>% round(2)
apply(postF1, 2, quantile, probs=c(0.025, 0.975)) %>% round(2)
```



### Balanced Acc histogram 
```{r}
postAcc <- lapply(cmx, function(x) x$byClass %>%  data.frame() %>%  select(Balanced.Accuracy) %>% t)
postAcc <- do.call(rbind, postAcc) 
colnames(postAcc) <-  className

postAccT <- postAcc %>% 
  data.frame() %>% 
  tidyr::gather(Activity_type, value)

postAccT <- postAccT %>% 
  dplyr::mutate(Activity_type = factor(Activity_type, levels=className, labels=className)) %>% 
  data.frame()

ggplot(data = postAccT, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ Activity_type, ncol = 1)  + 
  theme_classic() + 
  ggtitle("A") + xlab("Posterior distribution of balanced accuracy")

# Compute the posterior summary - mean and interval
colMeans(postAcc) %>%  round(2)
apply(postAcc, 2, median) %>% round(2)
apply(postAcc, 2, quantile, probs=c(0.025, 0.975)) %>% round(2)

```



### Brier score (not quite useful score for imbalanced class), not used in the paper 
```{r}
#Split by MCMC sample and compute CMX in each sample  
bDtSplitBrierTrue <-  lapply(bDtSplit, function(x) x %>% select(obsClassLabel))
bDtSplitBrierProb <-  lapply(bDtSplit, function(x) x %>% select(starts_with("Class")) %>%  as.matrix)
bDtSplitBrierProb <- lapply(bDtSplitBrierProb, function(x){ colnames(x) <- bDtSplitBrierTrue[[1]]$obsClassLabel %>% levels(); return(x)})
  
funcBrier <- function(x, y){return(mlr3measures::mbrier(truth = y$obsClassLabel, prob = x))}

hist(mapply(funcBrier, bDtSplitBrierProb, bDtSplitBrierTrue))


# Each MCMC has table (confusion matrix)
cB <- mclapply(bDtSplit, funcCMX)
# Concert CX table into df 
cB <- mclapply(cB, function(x) x$table %>% data.frame) 

# Combine all cMX , each CX data frame has a column of prediciton label, ref label, and pairwise frequency (match)
cB <- cB %>% reduce(left_join, by = c("Prediction","Reference"))
# columne to n = 6*6 classes, m = 2000 itr + prediction label, reference label
colnames(c) <- c("Prediction","Reference", 1:{ncol(c)-2})
```
 

### Calib score 
```{r}

# function to compute calibration 
funcCalib <- function(d){
  predictProb <- d$predictProb
  targetClass <- d$targetClass  
  #bin <- cut(predictProb, breaks = quantile(predictProb, probs = seq(0, 1, 0.02)), include.lowest = TRUE)
  bin <- cut(predictProb, breaks = seq(0, 1, by = 0.1), include.lowest = TRUE, right= FALSE)
  ##binPoint <- seq(0.05, 1, by = 0.1) 
  calibration_table <- aggregate(cbind(targetClass, predictProb) ~ bin, 
                                 data.frame(targetClass, predictProb, bin), 
                                 function(x) c(mean = mean(x)))
  ##calibration_table$binPoint <- binPoint
  return(calibration_table)
}

# Function to get intercept and slope of the calibratin curve 
funcCalibSummary<- function(dCalib){
  fit <- lm(dCalib$targetClass ~ dCalib$predictProb)
  as.numeric(c(fit$coefficients[1], fit$coefficients[2]))
}



dLCalib = list()
# Get prediction and response for a particular class for each sample 
dLCalib[[1]] <- lapply(bDtSplit, function(x) data.frame(predictProb = x$Class1, targetClass = ifelse(x$obsClass == 1, 1, 0)))
dLCalib[[2]] <- lapply(bDtSplit, function(x) data.frame(predictProb = x$Class2, targetClass = ifelse(x$obsClass == 2, 1, 0)))
dLCalib[[3]] <- lapply(bDtSplit, function(x) data.frame(predictProb = x$Class3, targetClass = ifelse(x$obsClass == 3, 1, 0)))
dLCalib[[4]] <- lapply(bDtSplit, function(x) data.frame(predictProb = x$Class4, targetClass = ifelse(x$obsClass == 4, 1, 0)))
dLCalib[[5]] <- lapply(bDtSplit, function(x) data.frame(predictProb = x$Class5, targetClass = ifelse(x$obsClass == 5, 1, 0)))
dLCalib[[6]] <- lapply(bDtSplit, function(x) data.frame(predictProb = x$Class6, targetClass = ifelse(x$obsClass == 6, 1, 0)))


par(mfrow = c(3,2))
j = 1
for(j in 1:6){
  dL <- dLCalib[[j]]
  
  # Get calibration binned outputs and summary slope and intercept
  calibOut <- lapply(dL, funcCalib)
  calibS <- lapply(calibOut, funcCalibSummary)
  
  # Get figure labels 
  postIntercept <- round(c(
    mean(do.call(rbind, calibS)[,1]), 
    quantile(do.call(rbind, calibS)[,1], probs = c(0.025)), 
    quantile(do.call(rbind, calibS)[,1], probs = c(0.975)) 
  ), 3)
  postIntercept <- paste("Intercept: ", postIntercept[1], "  (", postIntercept[2], ", ", postIntercept[3], ")", sep ="")
  
  postSlope <- round(c(
    mean(do.call(rbind, calibS)[,2]), 
    quantile(do.call(rbind, calibS)[,2], probs = c(0.025)), 
    quantile(do.call(rbind, calibS)[,2], probs = c(0.975)) 
  ), 3)
  postSlope <- paste("Slope: ", postSlope[1], "  (", postSlope[2], ", ", postSlope[3], ")", sep ="")
  
  # Plot calibration 
  plot(y=calibOut[[1]]$targetClass, 
       ##x=calibOut[[1]]$binPoint, 
       x=calibOut[[1]]$predictProb, 
       type="l", 
       col=alpha(rgb(0,0,0), 0.1), 
       ylim = c(0,1), 
       xlim = c(0,1), 
       xlab = "Predicted", 
       ylab = "Observed", 
       cex.lab=1.3, cex.axis=1.5, cex.main=1.5, cex.sub=1.5
    )
  abline(a = 0, b = 1)
  title( adj = 0, LETTERS[j])
  text(0.35, 0.95, postIntercept, cex = 1.4)
  text(0.35, 0.85, postSlope, cex = 1.4)
  
  for (i in 1:length(calibOut)){
    lines(y=calibOut[[i]]$targetClass, x=calibOut[[1]]$predictProb, type="l", col=alpha(rgb(0,0,0), 0.01))
  }
  cat("Class ", j , "  \n")
}


dev.off()



d <- bDtSplit[[2]]
predictProb <- d$Class1
targetClass <- ifelse(d$obsClass == 1, 1, 0)  
table(targetClass)
calPerf = val.prob( p = predictProb, y = targetClass)

calPerf$CalibrationCurves$FlexibleCalibration
plot(x=calPerf$CalibrationCurves$FlexibleCalibration$x, y = calPerf$CalibrationCurves$FlexibleCalibration$y)


```








### Gender for each class 
```{r, fig.width=12, fig.height=6}
#ID for females and males, join demographic table with data.table of test data
female <- demogTable %>% filter(gender == "Female") %>% select(participant_id) %>% unlist
male <- demogTable %>% filter(gender == "Male") %>% select(participant_id) %>% unlist


# Compute by gender 
bind_rows(
  funcGetPostAccClass(bDt[id %in% female, ]), 
  funcGetPostAccClass(bDt[id %in% male, ])  
) %>%  
addHtmlTableStyle(css.cell = rep("width: 80;", 4)) %>%
htmlTable( 
  tspanner = c("Female", "Male"),
  n.tspanner = c(6), 
  caption = "Accuracy by gender, per class", 
  ctable = c("solid", "double"))
```




### Per person, inbalanced metric aggregated over all classes     
```{r, fig.height=7, fig.width=4}
# split by person 

bDtSplitPerson <- split(bDt, list(bDt$id))
#bDtSplitPerson <- split(bDt, list(bDt$id)) 

funcCMX <- function(x){return(confusionMatrix(factor(x$maxProbClass), factor(x$obsClass), mode="everything"))}


meanVec <- rep(NA, length(bDtSplitPerson))
CIlowerVec <-  rep(NA, length(bDtSplitPerson))
CIUpperVec <- rep(NA, length(bDtSplitPerson))
IDVec <- names(bDtSplitPerson)


for(i in 1:length(bDtSplitPerson)){
  a <- split(bDtSplitPerson[[i]], list(bDtSplitPerson[[i]]$mcmcItr))
  aCx <- mclapply(a, funcCMX, mc.cores = 20)
  aOut <- lapply(aCx, function(x) x$byClass %>%  data.frame() %>%  select(Balanced.Accuracy) %>% t)
  
  aOut <- do.call(rbind, aOut) 
  colnames(aOut) <-  className
  postMeanClass <- apply(aOut, 2, median) 
  postCIClass <- apply(aOut, 2, quantile, probs=c(0.025, 0.975)) 
  meanVec[i] <- mean(postMeanClass)
  CIlowerVec[i] <- mean(postCIClass[1,])
  CIUpperVec[i] <- mean(postCIClass[2,])
  cat("person", i, "\n")
}


# Compute per-sperson acc
acc <- data.frame(IDVec, meanVec, CIlowerVec, CIUpperVec)
acc$IDVec <- factor(1:37)

ggplot(acc, aes(x = IDVec, y = meanVec))+
  geom_col() +
  geom_errorbar(aes(x=IDVec, ymin=CIlowerVec, ymax=CIUpperVec)) +
  xlab("Participant") + 
  ylab("Balanced accuracy (95% Credible Interval)") + 
  theme
```





### Histogram for probability distribution of classes for some time points 
The y axis is truncated. 
```{r fig.height=3, fig.width=10, warning=FALSE}

# Get a model object from one of the participants    
b_example <- readRDS("data_output/BART_PostSimTestTrain__ndPost_4000_Thin_500_Core_8_xTrain__numTrace_4_nSkip_3000_bartType_1_note_noScaling_saveTime___17_19:57_dataSource_5_loc_pock_ID_1_Scale_1_dropCoorFeatures_1_nTree_50loocv_0.rds")

# Density - not needed 
#funcPlotDensity <- function(timeid, personid, df, bin, yMax = 30){
# ti <- df[obsID == timeid & id == personid, ]
#  ti_long <- melt(ti, 
#                  id.vars =  c("mcmcItr", "obsID", "obsClass", "obsClassLabel"),
#       measure.vars = className, 
#       variable.name = "class", 
#       value.name = "Probability")
#  
#  levels(ti_long$class) <- c("Lying", "Sitting", "Walking", "Running_3METs", "Running_5METs", "Running_7METs")
  
#  ggplot(ti_long, aes(x=Probability, fill=class)) + 
#  geom_density(alpha=0.3,adjust = 2,  position="identity") + 
#    coord_cartesian(ylim=c(0, yMax))
#}

# Used in the function below 
className <- paste("Class", 1:6, sep = "")
# Function is in funcBARTPosterior.R
# Y-AXIS is truncated 
funcPlotHistPost(timeid = 200, personid = unique(b_example[[2]]$id), df = b_example[[2]], bin =0.025, yMax = 500)  

funcPlotHistPost(timeid = 1, personid = unique(b_example[[2]]$id), df = b_example[[2]], bin =0.025, yMax = 500)  
```







#-------Random forest ------------------------------------
### PREP
```{r data_rf}
# Name files ----------------------------------------------------------------------------------
# Named list of filename for post out generated by manual and parallel runs 
listRFFile_parallel <- list.files("data_output/Parallel", pattern="BART__ndPost", full.names=TRUE)

# Remove files with certain character, such as manual run 
# listPostFile_parallel <- listPostFile[-grep("MANUAL", listPostFile)]

# List IDS from files - order is changed 
nameIndex<- str_replace(listRFFile_parallel, '.+loocv_(.+)', '\\1') %>% 
  str_replace('(.+).rds', '\\1') 

# Duplicated person TD 
nameIndex[str_remove(nameIndex, "_ArrayIndex_\\d+") %>%  duplicated ]

# Load data ------------------------------------------------------
bool_loadPostFiles = FALSE
if(bool_loadPostFiles){ # load individual files and combine 
  r <- list()
  for(i in 1:length(listRFFile_parallel)){
    cat(c("loading -----", listRFFile_parallel[[i]]), "---------", "\n", i ,"\n")
    a <- readRDS(listRFFile_parallel[[i]])
    r[[i]] <- a$resultTestRF
  }
  names(r) <- listRFFile_parallel
  saveRDS(r, "data_output/Parallel/b_RF.rds")
}else{# load already combined file 
  #saveRDS(r, "data_output/Parallel/summary/b_RF.rds")
  r <- readRDS("data_output/Parallel/b_RF.rds")
}



# Names ---------------------------------------------
# check files and shorten list name - one person was accidentally duplicated 
rNamesIndex <- names(r) %>% str_replace('.+loocv_(.+)', '\\1') %>% 
  str_replace('(.+).rds', '\\1') 
names(r) <- rNamesIndex

# duplicated person ID 
str_remove(rNamesIndex, "_ArrayIndex_\\d+") %>%  
  duplicated %>%  
  sum
rNamesIndex[
  str_remove(rNamesIndex, "_ArrayIndex_\\d+") %>%  duplicated]


# Create summary stats data frame across all prediction/observation pairs 
nameDf <- data.frame(
  classLabel = unique(bDt$obsClassLabel), 
  id = factor(1:6)
  )
```


### Acc (not balanced accuracy) metric, comparison between RF and bound RF
```{r}
# person specific values 
c <- mclapply(r, function(x) x$results$RF$cf_matrix$byClass %>% data.frame) 
c <- c %>% reduce(left_join, by = c("Prediction","Reference"))
colnames(c) <- c("Prediction","Reference", 1:{ncol(c)-2})

# combine all data 
c <- mclapply(r, function(x) x$results$RF$cf_matrix$table %>% data.frame) 
c <- c %>% reduce(left_join, by = c("Prediction","Reference"))
colnames(c) <- c("Prediction","Reference", 1:{ncol(c)-2})

bind_cols(c[,1:2], Sum = rowSums(c[, 3:ncol(c)])) %>% 
  mutate(match = ifelse(Prediction == Reference, "Match", "noMatch")) %>%  
  group_by(Reference, match) %>%  
  summarise(count = sum(Sum)) %>% 
  spread(match, count) %>%  
  mutate(accuracy_RF = round(Match/(Match + noMatch),2)) %>% 
  bind_cols(nameDf) %>%
  select(classLabel, accuracy_RF) %>% 
  bind_cols(funcGetPostAccClass(bDt)) %>% 
  select(-c(Reference,classLabel)) 
```


### Balanced Accuracy, manual computation as (sens + spec) / 2 for each class 
```{r}

funcGetSensSpec <- function(varClass, dat){
  totalPos <-  dat %>%  filter(Reference == varClass) %>% summarise(count = sum(Sum))
  totalPPos <- dat %>%  filter(Reference == varClass & Prediction == varClass) %>% summarise(count = sum(Sum))
  Sens <- totalPPos/totalPos
  
  totalNeg <-  dat %>%  filter(Reference != varClass) %>% summarise(count = sum(Sum))
  totalPNeg <- dat %>%  filter(Prediction != varClass & Reference != varClass) %>% summarise(count = sum(Sum))
  Spec <- totalPNeg/totalNeg
  
  return(as.numeric(c(Sens, Spec)))
}

c <- mclapply(r, function(x) x$results$RF$cf_matrix$table %>% data.frame) 
c <- c %>% reduce(left_join, by = c("Prediction","Reference"))
confMx <- bind_cols(c[,1:2], Sum = rowSums(c[, 3:ncol(c)])) 

(funcGetSensSpec("L", confMx)[1] + funcGetSensSpec("L", confMx)[2])/2
(funcGetSensSpec("S", confMx)[1] + funcGetSensSpec("S", confMx)[2])/2
(funcGetSensSpec("W", confMx)[1] + funcGetSensSpec("W", confMx)[2])/2
(funcGetSensSpec("R3", confMx)[1] + funcGetSensSpec("R3", confMx)[2])/2
(funcGetSensSpec("R5", confMx)[1] + funcGetSensSpec("R5", confMx)[2])/2
(funcGetSensSpec("R7", confMx)[1] + funcGetSensSpec("R7", confMx)[2])/2
```


### F1 score, manual computation from precision and recall 
```{r}
funcGetF1 <- function(varClass, dat){
#funcGetSensSpec <- function(ID, varClass, dat){
  TP <- dat %>%  filter(Reference == varClass & Prediction == varClass) %>% summarise(count = sum(Sum))
  FP <- dat %>%  filter(Reference != varClass & Prediction == varClass) %>% summarise(count = sum(Sum))
  FN <- dat %>%  filter(Reference == varClass & Prediction != varClass) %>% summarise(count = sum(Sum))
  
Prec <- TP/(FP + TP)
Rec <- TP/(TP + FN)  
return( (2*TP)/(2*TP + FP + FN))
}

c <- mclapply(r, function(x) x$results$RF$cf_matrix$table %>% data.frame) 
c <- c %>% reduce(left_join, by = c("Prediction","Reference"))
confMx <- bind_cols(c[,1:2], Sum = rowSums(c[, 3:ncol(c)])) 
  
funcGetF1("L", confMx)
funcGetF1("S", confMx)
funcGetF1("W", confMx)
funcGetF1("R3", confMx)
funcGetF1("R5", confMx)
funcGetF1("R7", confMx)
```


### Confusion matrix - classificaiton by random forest 
```{r}
c <- mclapply(r, function(x) x$results$RF$cf_matrix$table %>% data.frame) 
c <- c %>% reduce(left_join, by = c("Prediction","Reference"))
colnames(c) <- c("Prediction","Reference", 1:{ncol(c)-2})

cSummary <- bind_cols(c[,1:2], Sum = rowSums(c[, 3:ncol(c)])) %>% 
  mutate(match = ifelse(Prediction == Reference, "Match", "noMatch")) 

# Posterior mean - standard Mx, except cell values are sum of probabilities, not 1/0 - probably two show very similar results 
cSummary %>%
  select(Prediction, Reference, Sum) %>% 
  mutate(Mean = round(Sum, 2)) %>% 
  ggplot2::ggplot(aes(Prediction, Reference)) +
  geom_tile(aes(fill = Sum), colour = "gray50") +
  scale_fill_gradient(low = "white", high = muted("gray80")) +
  geom_text(aes(label = Sum)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Agreement of classification, ") 

rm(r)
```

